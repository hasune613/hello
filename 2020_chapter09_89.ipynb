{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020_chapter09_89.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyIcTbtyG1f2LpxREMmTVN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasune613/hello-world/blob/main/2020_chapter09_89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyeIeJSTgMXu",
        "outputId": "2c6b7eeb-d885-4b78-a6da-6e59b37cf6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-08 04:41:53--  https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29224203 (28M) [application/x-httpd-php]\n",
            "Saving to: ‘NewsAggregatorDataset.zip.2’\n",
            "\n",
            "NewsAggregatorDatas 100%[===================>]  27.87M  32.8MB/s    in 0.8s    \n",
            "\n",
            "2022-01-08 04:41:54 (32.8 MB/s) - ‘NewsAggregatorDataset.zip.2’ saved [29224203/29224203]\n",
            "\n",
            "Archive:  NewsAggregatorDataset.zip\n",
            "replace 2pageSessions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# データのダウンロード\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip\n",
        "!unzip NewsAggregatorDataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 行数の確認\n",
        "!wc -l ./newsCorpora.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CehBHeCygTZK",
        "outputId": "c3dc0a01-6f51-400c-e65d-d5fc50c4ffb8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "422937 ./newsCorpora.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 先頭10行の確認\n",
        "!head -10 ./newsCorpora.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrrjGbo5gamb",
        "outputId": "a3876734-6152-4562-b191-30d7c69d0dbc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\tFed official says weak data caused by weather, should not slow taper\thttp://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\?track=rss\tLos Angeles Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.latimes.com\t1394470370698\n",
            "2\tFed's Charles Plosser sees high bar for change in pace of tapering\thttp://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html\tLivemint\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.livemint.com\t1394470371207\n",
            "3\tUS open: Stocks fall after Fed official hints at accelerated tapering\thttp://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371550\n",
            "4\tFed risks falling 'behind the curve', Charles Plosser says\thttp://www.ifamagazine.com/news/fed-risks-falling-behind-the-curve-charles-plosser-says-294430\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371793\n",
            "5\tFed's Plosser: Nasty Weather Has Curbed Job Growth\thttp://www.moneynews.com/Economy/federal-reserve-charles-plosser-weather-job-growth/2014/03/10/id/557011\tMoneynews\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.moneynews.com\t1394470372027\n",
            "6\tPlosser: Fed May Have to Accelerate Tapering Pace\thttp://www.nasdaq.com/article/plosser-fed-may-have-to-accelerate-tapering-pace-20140310-00371\tNASDAQ\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.nasdaq.com\t1394470372212\n",
            "7\tFed's Plosser: Taper pace may be too slow\thttp://www.marketwatch.com/story/feds-plosser-taper-pace-may-be-too-slow-2014-03-10\\?reflink=MW_news_stmp\tMarketWatch\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.marketwatch.com\t1394470372405\n",
            "8\tFed's Plosser expects US unemployment to fall to 6.2% by the end of 2014\thttp://www.fxstreet.com/news/forex-news/article.aspx\\?storyid=23285020-b1b5-47ed-a8c4-96124bb91a39\tFXstreet.com\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.fxstreet.com\t1394470372615\n",
            "9\tUS jobs growth last month hit by weather:Fed President Charles Plosser\thttp://economictimes.indiatimes.com/news/international/business/us-jobs-growth-last-month-hit-by-weatherfed-president-charles-plosser/articleshow/31788000.cms\tEconomic Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\teconomictimes.indiatimes.com\t1394470372792\n",
            "10\tECB unlikely to end sterilisation of SMP purchases - traders\thttp://www.iii.co.uk/news-opinion/reuters/news/152615\tInteractive Investor\tb\tdPhGU51DcrolUIMxbRm0InaHGA2XM\twww.iii.co.uk\t1394470501265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 読込時のエラー回避のためダブルクォーテーションをシングルクォーテーションに置換\n",
        "!sed -e 's/\"/'\\''/g' ./newsCorpora.csv > ./newsCorpora_re.csv"
      ],
      "metadata": {
        "id": "0onWX29ogbnC"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データの読込\n",
        "df = pd.read_csv('./newsCorpora_re.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
        "\n",
        "# データの抽出\n",
        "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
        "\n",
        "# データの分割\n",
        "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
        "train.reset_index(drop=True, inplace=True)\n",
        "valid.reset_index(drop=True, inplace=True)\n",
        "test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 事例数の確認\n",
        "print('【学習データ】')\n",
        "print(train['CATEGORY'].value_counts())\n",
        "print('【検証データ】')\n",
        "print(valid['CATEGORY'].value_counts())\n",
        "print('【評価データ】')\n",
        "print(test['CATEGORY'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5MQbTRxgeEi",
        "outputId": "d42c581b-38f2-4632-bf17-b6a78b1b828a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "【学習データ】\n",
            "b    4501\n",
            "e    4235\n",
            "t    1220\n",
            "m     728\n",
            "Name: CATEGORY, dtype: int64\n",
            "【検証データ】\n",
            "b    563\n",
            "e    529\n",
            "t    153\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n",
            "【評価データ】\n",
            "b    563\n",
            "e    530\n",
            "t    152\n",
            "m     91\n",
            "Name: CATEGORY, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['TITLE']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo9Qxsxqm7Yh",
        "outputId": "e3df13c8-db12-4a80-e418-b7bf500046ae"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        REFILE-UPDATE 1-European car sales up for sixt...\n",
              "1        Amazon Plans to Fight FTC Over Mobile-App Purc...\n",
              "2        Kids Still Get Codeine In Emergency Rooms Desp...\n",
              "3        What On Earth Happened Between Solange And Jay...\n",
              "4        NATO Missile Defense Is Flight Tested Over Hawaii\n",
              "                               ...                        \n",
              "10679                      The Pillow Book of Tim Geithner\n",
              "10680    Robin Thicke spends $20000 on Amethyst stone a...\n",
              "10681    Japanese Shares Little Changed After Biggest R...\n",
              "10682    WRAPUP 1-Liberia shuts schools as Ebola spread...\n",
              "10683    Justin Bieber Caught Using N-Word & Joining KK...\n",
              "Name: TITLE, Length: 10684, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "DDExso1sgg-Z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import optim\n",
        "from torch import cuda\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "GT1qHtH5gjPx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Datasetの定義\n",
        "class CreateDataset(Dataset):\n",
        "  def __init__(self, X, y, tokenizer, max_len):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):  # len(Dataset)で返す値を指定\n",
        "    return len(self.y)\n",
        "\n",
        "  def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
        "    text = self.X[index]\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "      text,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      pad_to_max_length=True\n",
        "    )\n",
        "    ids = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "\n",
        "    return {\n",
        "      'ids': torch.LongTensor(ids),\n",
        "      'mask': torch.LongTensor(mask),\n",
        "      'labels': torch.Tensor(self.y[index])\n",
        "    }"
      ],
      "metadata": {
        "id": "q7o2yaSShCdR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解ラベルのone-hot化\n",
        "y_train = pd.get_dummies(train, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_valid = pd.get_dummies(valid, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "y_test = pd.get_dummies(test, columns=['CATEGORY'])[['CATEGORY_b', 'CATEGORY_e', 'CATEGORY_t', 'CATEGORY_m']].values\n",
        "\n",
        "# Datasetの作成\n",
        "max_len = 20\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "dataset_train = CreateDataset(train['TITLE'], y_train, tokenizer, max_len)\n",
        "dataset_valid = CreateDataset(valid['TITLE'], y_valid, tokenizer, max_len)\n",
        "dataset_test = CreateDataset(test['TITLE'], y_test, tokenizer, max_len)\n",
        "\n",
        "\n",
        "for var in dataset_train[0]:\n",
        "    print(f'{var}:{dataset_train[0][var]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZcd4Lu1hDty",
        "outputId": "b2e83a1f-dca7-438e-e1ac-e167b912eff8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ids:tensor([  101, 25416,  9463,  1011, 10651,  1015,  1011,  2647,  2482,  4341,\n",
            "         2039,  2005,  4369,  3204,  2004, 18730,  8980,   102,     0,     0])\n",
            "mask:tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
            "labels:tensor([1., 0., 0., 0.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT分類モデルの定義\n",
        "class BERTClass(torch.nn.Module):\n",
        "  def __init__(self, drop_rate, otuput_size):\n",
        "    super().__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "    self.drop = nn.Dropout(drop_rate)\n",
        "    self.fc = nn.Linear(768, otuput_size)\n",
        "    \n",
        "  def forward(self, ids, mask):\n",
        "    _, out = self.bert(ids, attention_mask=mask)\n",
        "    out = self.fc(self.drop(out))\n",
        "    return out"
      ],
      "metadata": {
        "id": "WUsWP-2MhDnI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_and_accuracy(model, criterion, loader, device):\n",
        "  \"\"\" 損失・正解率を計算\"\"\"\n",
        "  model.eval()\n",
        "  loss = 0.0\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播\n",
        "      outputs = model(ids, mask)\n",
        "\n",
        "      # 損失計算\n",
        "      loss += criterion(outputs, labels).item()\n",
        "\n",
        "      # 正解率計算\n",
        "      pred = torch.argmax(outputs, dim=-1).cpu().numpy() # バッチサイズの長さの予測ラベル配列\n",
        "      labels = torch.argmax(labels, dim=-1).cpu().numpy()  # バッチサイズの長さの正解ラベル配列\n",
        "      total += len(labels)\n",
        "      correct += (pred == labels).sum().item()\n",
        "      \n",
        "  return loss / len(loader), correct / total\n",
        "  \n",
        "\n",
        "def train_model(dataset_train, dataset_valid, batch_size, model, criterion, optimizer, num_epochs, device=None):\n",
        "  \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
        "  # デバイスの指定\n",
        "  model.to(device)\n",
        "\n",
        "  # dataloaderの作成\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "  dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
        "\n",
        "  # 学習\n",
        "  log_train = []\n",
        "  log_valid = []\n",
        "  for epoch in range(num_epochs):\n",
        "    # 開始時刻の記録\n",
        "    s_time = time.time()\n",
        "\n",
        "    # 訓練モードに設定\n",
        "    model.train()\n",
        "    for data in dataloader_train:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 勾配をゼロで初期化\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 順伝播 + 誤差逆伝播 + 重み更新\n",
        "      outputs = model(ids, mask)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    # 損失と正解率の算出\n",
        "    loss_train, acc_train = calculate_loss_and_accuracy(model, criterion, dataloader_train, device)\n",
        "    loss_valid, acc_valid = calculate_loss_and_accuracy(model, criterion, dataloader_valid, device)\n",
        "    log_train.append([loss_train, acc_train])\n",
        "    log_valid.append([loss_valid, acc_valid])\n",
        "\n",
        "    # チェックポイントの保存\n",
        "    torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, f'checkpoint{epoch + 1}.pt')\n",
        "\n",
        "    # 終了時刻の記録\n",
        "    e_time = time.time()\n",
        "\n",
        "    # ログを出力\n",
        "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, accuracy_train: {acc_train:.4f}, loss_valid: {loss_valid:.4f}, accuracy_valid: {acc_valid:.4f}, {(e_time - s_time):.4f}sec') \n",
        "\n",
        "  return {'train': log_train, 'valid': log_valid}"
      ],
      "metadata": {
        "id": "sks8jMW4hDj0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの設定\n",
        "DROP_RATE = 0.4\n",
        "OUTPUT_SIZE = 4\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# モデルの定義\n",
        "model = BERTClass(DROP_RATE, OUTPUT_SIZE)\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "# オプティマイザの定義\n",
        "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# デバイスの指定\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# モデルの学習\n",
        "log = train_model(dataset_train, dataset_valid, BATCH_SIZE, model, criterion, optimizer, NUM_EPOCHS, device=device)"
      ],
      "metadata": {
        "id": "YLMKfTTnhDgo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ログの可視化\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax[0].plot(np.array(log['train']).T[0], label='train')\n",
        "ax[0].plot(np.array(log['valid']).T[0], label='valid')\n",
        "ax[0].set_xlabel('epoch')\n",
        "ax[0].set_ylabel('loss')\n",
        "ax[0].legend()\n",
        "ax[1].plot(np.array(log['train']).T[1], label='train')\n",
        "ax[1].plot(np.array(log['valid']).T[1], label='valid')\n",
        "ax[1].set_xlabel('epoch')\n",
        "ax[1].set_ylabel('accuracy')\n",
        "ax[1].legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i0l-OkVOhDdh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解率の算出\n",
        "def calculate_accuracy(model, dataset, device):\n",
        "  # Dataloaderの作成\n",
        "  loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
        "\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      # デバイスの指定\n",
        "      ids = data['ids'].to(device)\n",
        "      mask = data['mask'].to(device)\n",
        "      labels = data['labels'].to(device)\n",
        "\n",
        "      # 順伝播 + 予測値の取得 + 正解数のカウント\n",
        "      outputs = model.forward(ids, mask)\n",
        "      pred = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "      labels = torch.argmax(labels, dim=-1).cpu().numpy()\n",
        "      total += len(labels)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "  return correct / total\n",
        "\n",
        "print(f'正解率（学習データ）：{calculate_accuracy(model, dataset_train, device):.3f}')\n",
        "print(f'正解率（検証データ）：{calculate_accuracy(model, dataset_valid, device):.3f}')\n",
        "print(f'正解率（評価データ）：{calculate_accuracy(model, dataset_test, device):.3f}')"
      ],
      "metadata": {
        "id": "buzG9F_ahDaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DIg91LbYhDXt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}