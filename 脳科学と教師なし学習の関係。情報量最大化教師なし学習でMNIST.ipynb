{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "脳科学と教師なし学習の関係。情報量最大化教師なし学習でMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnZcBrv8RrNDfw/JstBwQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hasune613/hello-world/blob/main/%E8%84%B3%E7%A7%91%E5%AD%A6%E3%81%A8%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92%E3%81%AE%E9%96%A2%E4%BF%82%E3%80%82%E6%83%85%E5%A0%B1%E9%87%8F%E6%9C%80%E5%A4%A7%E5%8C%96%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%AD%A6%E7%BF%92%E3%81%A7MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpcIUi277cmJ"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFx3pPo_7cpb",
        "outputId": "e8551b7c-9f46-4fe3-b5d2-94ee15f6d2c1"
      },
      "source": [
        "seed_value = 0\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "random.seed(seed_value)\n",
        "torch.manual_seed(seed_value) # PyTorchを使う場合"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3bbe594730>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rw7FoiW7ctL",
        "outputId": "a0972335-7aa1-4ca2-a02e-954ba6eb3e4b"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# GPUを使用。cudaと出力されるのを確認。"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OykOJ0g7cwn"
      },
      "source": [
        "# MNISTの画像をダウンロードし、DataLoaderにする（TrainとTest）\n",
        "from torchvision import datasets , transforms\n",
        "\n",
        "batch_size_train = 512\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.',train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                   ])),\n",
        "                   batch_size = batch_size_train,shuffle=True, drop_last= True)\n",
        "# drop_lastは最後のミニバッチが規定のサイズより小さい場合は使用しない設定\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('.',train=False, transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        ])),\n",
        "        batch_size = 1024,shuffle=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH788KjV7czt"
      },
      "source": [
        "# ディープラーニングモデル\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "OVER_CLUSTRING_Rate = 10\n",
        "# 最終出力層は性能upを見込み、10種類に加え、さらに多くに分類させる（overclustring）\n",
        "# 最終出力層は10種類版とoverclustering版。損失関数もその両方のアウトプットを計算して総和を使用。\n",
        "\n",
        "class NetIIC(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetIIC,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1,128,5,2,bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(128,128,5,1,bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(128,128,5,1,bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.conv4 = nn.Conv2d(128,256,4,1,bias = False)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # 0-9に対応すると期待したい10種類のクラス\n",
        "        self.fc = nn.Linear(256,10)\n",
        "\n",
        "        # overclustering\n",
        "        # 実際の想定よりも多めにクラスタリングさせることで、ネットワークで微細な変化を捉えられるようにする\n",
        "        self.fc_overclustering = nn.Linear(256,10*OVER_CLUSTRING_Rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x_prefinal = x.view(x.size(0),-1)\n",
        "        y = F.softmax(self.fc(x_prefinal),dim=1)\n",
        "\n",
        "        y_overclustering = F.softmax(self.fc_overclustering(x_prefinal),dim=1)# overclustering\n",
        "\n",
        "        return y,y_overclustering\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4pt_fnI7c68"
      },
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def weight_init(m):\n",
        "     \"\"\"重み初期化\"\"\"\n",
        "     if isinstance(m,nn.Conv2d):\n",
        "         init.xavier_normal_(m.weight.data)\n",
        "         if m.bias is not None:\n",
        "             init.normal_(m.bias.data)\n",
        "     elif isinstance(m,  nn.BatchNorm2d):\n",
        "         init.normal_(m.weight.data, mean=1,std=0.02)\n",
        "         init.constant_(m.bias.data,0)\n",
        "     elif isinstance(m, nn.Linear):\n",
        "       \n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9Rmv7iZ7c-U"
      },
      "source": [
        "import torchvision as tv\n",
        "import torchvision.transforms.functional as TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQlItj6Ytkrx"
      },
      "source": [
        "def perturb_imagedata(x):\n",
        "    y = x.clone()\n",
        "    batch_size = x.size(0)\n",
        "\n",
        "    # ランダムなアフィン変換を実施\n",
        "    trans = tv.transforms.RandomAffine(15,(0.2,0.2,),(0.2,0.75,))\n",
        "    for i in range(batch_size):\n",
        "        y[i,0] = TF.to_tensor(trans(TF.to_pil_image(y[i,0])))\n",
        "\n",
        "    noise = torch.randn(batch_size,1,x.size(2),x.size(3))\n",
        "    div = torch.randint(20,30,(batch_size,),\n",
        "                        dtype = torch.float32).view(batch_size,1,1,1)\n",
        "    y += noise / div\n",
        "\n",
        "    return y\n",
        "                    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUAl1cFjtkif"
      },
      "source": [
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEgjoPx_tkfU"
      },
      "source": [
        "def compute_joint(x_out, x_tf_out):\n",
        "    # x_out、x_tf_outは torch.Size([512, 10])。この二つをかけ算して同時分布を求める、torch.Size([2048, 10, 10])にする。\n",
        "    # torch.Size([512, 10, 1]) * torch.Size([512, 1, 10])\n",
        "    p_i_j = x_out.unsqueeze(2) * x_tf_out.unsqueeze(1)\n",
        "    # p_i_j は　torch.Size([512, 10, 10])\n",
        "\n",
        "    # 全ミニバッチを足し算する ⇒ torch.Size([10, 10])\n",
        "    p_i_j = p_i_j .sum(dim=0)\n",
        "\n",
        "    # 転置行列と足し算して割り算（対称化） ⇒ torch.Size([10, 10])   \n",
        "    p_i_j = (p_i_j + p_i_j.t()) / 2.\n",
        "\n",
        "    # 規格化 ⇒ torch.Size([10, 10])\n",
        "    p_i_j = p_i_j / p_i_j.sum()\n",
        "\n",
        "    return p_i_j\n",
        "    #p_i_jは通常画像の判定出力10種類と、\n",
        "    #変換画像の判定10種類の100パターンに対して、\n",
        "    #全ミニバッチが100パターンのどれだったのかの確率分布表を示す\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eUXf9H7OIhf"
      },
      "source": [
        "def IID_loss(x_out, x_tf_out, EPS = sys.float_info.epsilon):\n",
        "    # torch.Size([512, 10])、後ろの10は分類数なので、overclusteringのときは100\n",
        "    bs,k = x_out.size()\n",
        "    p_i_j = compute_joint(x_out,x_tf_out)# torch.Size([10, 10])\n",
        "\n",
        "    #同時確率の分布表から、変換画像の10パターンをsumをして周辺化し、\n",
        "    #元画像だけの周辺確率の分布表を作る\n",
        "    p_i = p_i_j.sum(dim=1).view( k, 1).expand(k,  k)\n",
        "\n",
        "    #同時確率の分布表から、元画像の10パターンをsumをして周辺化し、\n",
        "    #変換画像だけの周辺確率の分布表を作る\n",
        "    p_j = p_i_j.sum(dim = 0).view(1,k).expand(k , k)\n",
        "\n",
        "    # 0に近い値をlogに入れると発散するので、避ける\n",
        "    p_i_j = torch.where(p_i_j < EPS, torch.tensor(\n",
        "        [EPS],device = p_i_j.device), p_i_j)\n",
        "    p_j = torch.where(p_j < EPS, torch.tensor([EPS], device=p_j.device),p_j)\n",
        "    p_i = torch.where(p_i < EPS, torch.tensor([EPS], device=p_j.device),p_i)\n",
        "\n",
        "    alpha = 2.0 #通常の相互情報量の計算はalphaは1\n",
        "    loss = -1 * (p_i_j *(torch.log(p_i_j) -alpha*\n",
        "                        torch.log(p_j) - alpha * torch.log(p_i))).sum()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxV_pCS7Qtb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe9fed6-1b6a-460f-e08d-d20a958e7ba6"
      },
      "source": [
        "total_epoch = 3\n",
        "\n",
        "model = NetIIC()\n",
        "model.apply(weight_init)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)\n",
        "\n",
        "def train (total_epoch, model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer, T_0=2, T_mult=2\n",
        "    )\n",
        "\n",
        "    for epoch in range(total_epoch):\n",
        "        for batch_idx , (data, target) in enumerate(train_loader):\n",
        "\n",
        "            scheduler.step()\n",
        "\n",
        "            data_perturb = perturb_imagedata(data)# ノイズを与える\n",
        "\n",
        "            data = data.to(device)\n",
        "            data_perturb = data_perturb.to(device)\n",
        "            # 最適化関数の初期化\n",
        "            optimizer.zero_grad()\n",
        "            # ニューラルネットワーク出力\n",
        "            output, output_overclustering = model(data)\n",
        "            output_perturb, output_perturb_overclustering = model(data_perturb)\n",
        "            # 最適化関数の初期化\n",
        "            loss1 = IID_loss(output, output_perturb)\n",
        "            loss2 = IID_loss(output_overclustering,\n",
        "                            output_perturb_overclustering)\n",
        "            loss = loss1 = loss2\n",
        "            # 損失を減らすように更新\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # ログ出力\n",
        "            if batch_idx % 10 ==0:\n",
        "                print(f'Train Epoch {epoch}:iter{batch_idx} -\\tLoss1:{loss1.item():.6f} - \\tLoss2:{loss2.item():.6f}-\\tLoss_total:{loss1.item()+loss2.item():.6f}')\n",
        "\n",
        "\n",
        "    return  model, optimizer\n",
        "\n",
        "model_trained, optimizer = train(\n",
        "    total_epoch, model , train_loader, optimizer, device\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch 0:iter0 -\tLoss1:-7.031949 - \tLoss2:-7.031949-\tLoss_total:-14.063898\n",
            "Train Epoch 0:iter10 -\tLoss1:-8.761274 - \tLoss2:-8.761274-\tLoss_total:-17.522549\n",
            "Train Epoch 0:iter20 -\tLoss1:-9.085955 - \tLoss2:-9.085955-\tLoss_total:-18.171909\n",
            "Train Epoch 0:iter30 -\tLoss1:-9.167633 - \tLoss2:-9.167633-\tLoss_total:-18.335266\n",
            "Train Epoch 0:iter40 -\tLoss1:-9.241488 - \tLoss2:-9.241488-\tLoss_total:-18.482975\n",
            "Train Epoch 0:iter50 -\tLoss1:-9.273063 - \tLoss2:-9.273063-\tLoss_total:-18.546125\n",
            "Train Epoch 0:iter60 -\tLoss1:-9.259229 - \tLoss2:-9.259229-\tLoss_total:-18.518457\n",
            "Train Epoch 0:iter70 -\tLoss1:-9.280230 - \tLoss2:-9.280230-\tLoss_total:-18.560459\n",
            "Train Epoch 0:iter80 -\tLoss1:-9.306996 - \tLoss2:-9.306996-\tLoss_total:-18.613993\n",
            "Train Epoch 0:iter90 -\tLoss1:-9.347582 - \tLoss2:-9.347582-\tLoss_total:-18.695164\n",
            "Train Epoch 0:iter100 -\tLoss1:-9.402233 - \tLoss2:-9.402233-\tLoss_total:-18.804466\n",
            "Train Epoch 0:iter110 -\tLoss1:-9.391451 - \tLoss2:-9.391451-\tLoss_total:-18.782902\n",
            "Train Epoch 1:iter0 -\tLoss1:-9.396040 - \tLoss2:-9.396040-\tLoss_total:-18.792080\n",
            "Train Epoch 1:iter10 -\tLoss1:-9.389491 - \tLoss2:-9.389491-\tLoss_total:-18.778982\n",
            "Train Epoch 1:iter20 -\tLoss1:-9.449245 - \tLoss2:-9.449245-\tLoss_total:-18.898491\n",
            "Train Epoch 1:iter30 -\tLoss1:-9.580542 - \tLoss2:-9.580542-\tLoss_total:-19.161083\n",
            "Train Epoch 1:iter40 -\tLoss1:-9.701725 - \tLoss2:-9.701725-\tLoss_total:-19.403450\n",
            "Train Epoch 1:iter50 -\tLoss1:-9.790767 - \tLoss2:-9.790767-\tLoss_total:-19.581533\n",
            "Train Epoch 1:iter60 -\tLoss1:-9.864801 - \tLoss2:-9.864801-\tLoss_total:-19.729603\n",
            "Train Epoch 1:iter70 -\tLoss1:-9.983265 - \tLoss2:-9.983265-\tLoss_total:-19.966530\n",
            "Train Epoch 1:iter80 -\tLoss1:-10.099072 - \tLoss2:-10.099072-\tLoss_total:-20.198143\n",
            "Train Epoch 1:iter90 -\tLoss1:-10.243756 - \tLoss2:-10.243756-\tLoss_total:-20.487513\n",
            "Train Epoch 1:iter100 -\tLoss1:-10.199550 - \tLoss2:-10.199550-\tLoss_total:-20.399099\n",
            "Train Epoch 1:iter110 -\tLoss1:-10.338228 - \tLoss2:-10.338228-\tLoss_total:-20.676456\n",
            "Train Epoch 2:iter0 -\tLoss1:-10.310340 - \tLoss2:-10.310340-\tLoss_total:-20.620680\n",
            "Train Epoch 2:iter10 -\tLoss1:-10.362930 - \tLoss2:-10.362930-\tLoss_total:-20.725861\n",
            "Train Epoch 2:iter20 -\tLoss1:-10.239068 - \tLoss2:-10.239068-\tLoss_total:-20.478136\n",
            "Train Epoch 2:iter30 -\tLoss1:-10.505112 - \tLoss2:-10.505112-\tLoss_total:-21.010223\n",
            "Train Epoch 2:iter40 -\tLoss1:-10.751776 - \tLoss2:-10.751776-\tLoss_total:-21.503551\n",
            "Train Epoch 2:iter50 -\tLoss1:-10.874242 - \tLoss2:-10.874242-\tLoss_total:-21.748484\n",
            "Train Epoch 2:iter60 -\tLoss1:-11.033889 - \tLoss2:-11.033889-\tLoss_total:-22.067778\n",
            "Train Epoch 2:iter70 -\tLoss1:-11.092234 - \tLoss2:-11.092234-\tLoss_total:-22.184467\n",
            "Train Epoch 2:iter80 -\tLoss1:-11.033740 - \tLoss2:-11.033740-\tLoss_total:-22.067480\n",
            "Train Epoch 2:iter90 -\tLoss1:-11.161293 - \tLoss2:-11.161293-\tLoss_total:-22.322586\n",
            "Train Epoch 2:iter100 -\tLoss1:-11.192324 - \tLoss2:-11.192324-\tLoss_total:-22.384647\n",
            "Train Epoch 2:iter110 -\tLoss1:-11.164515 - \tLoss2:-11.164515-\tLoss_total:-22.329029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTS8ddKrgLYY"
      },
      "source": [
        "# モデル分類のクラスターの結果を確認\n",
        "\n",
        "def test(model, device, train_loader):\n",
        "    model.eval()\n",
        "\n",
        "    out_targs = []\n",
        "    ref_targs = []\n",
        "    cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            cnt += 1\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "            outputs , outputs_overclustering = model(data)\n",
        "\n",
        "            out_targs.append(outputs.argmax(dim=1).cpu())\n",
        "            ref_targs.append(target.cpu())\n",
        "            \n",
        "    out_targs = torch.cat(out_targs)\n",
        "    ref_targs = torch.cat(ref_targs)\n",
        "\n",
        "    return out_targs.numpy() , ref_targs.numpy()\n",
        "\n",
        "out_targs, ref_targs = test(model_trained, device, train_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg3VtJBmmiwu"
      },
      "source": [
        "!pip install -q scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCsgItc5gLUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d7104-03be-48ca-f4cf-54643ab00662"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# 混同行列（的な）を作る\n",
        "matrix = np.zeros((10,10))\n",
        "\n",
        "# 縦に数字の0から9を、横に判定されたクラスの頻度表を作成\n",
        "for i in range(len(out_targs)):\n",
        "    row = ref_targs[i]\n",
        "    col = out_targs[i]\n",
        "    matrix[row][col] += 1\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(matrix)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1. 439.  37. 310.  66.   0.   3. 124.   0.   0.]\n",
            " [434. 658.   1.   0.   8.   3.  25.   0.   1.   5.]\n",
            " [  0. 725.   8.  61.  74.   6.  17.  76.  65.   0.]\n",
            " [ 11. 834.   0.   2.  15.  46.  91.   9.   1.   1.]\n",
            " [  1. 698.  11.   7.   9.  44.  28. 184.   0.   0.]\n",
            " [  8. 701.  72.   7.  32.  50.   9.  13.   0.   0.]\n",
            " [ 20.  77. 312.   8. 263.   3.  36. 238.   1.   0.]\n",
            " [  0. 528.   0.   5.  21. 189. 253.   8.   1.  23.]\n",
            " [ 37. 636. 102.  23.   2. 119.  32.  22.   1.   0.]\n",
            " [  5. 801.   3.  18.   6.  41.  17. 115.   1.   2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a0u1KnsgLSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f02b88-2c42-4a5f-ce73-0f3537e2311b"
      },
      "source": [
        "total_num = matrix.sum().sum()\n",
        "print(total_num)\n",
        "correct_num_list = matrix.max(axis = 0)\n",
        "\n",
        "print(correct_num_list)\n",
        "print(correct_num_list.sum())\n",
        "\n",
        "print('正解率：',correct_num_list.sum() / total_num*100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000.0\n",
            "[434. 834. 312. 310. 263. 189. 253. 238.  65.  23.]\n",
            "2921.0\n",
            "正解率： 29.21\n"
          ]
        }
      ]
    }
  ]
}